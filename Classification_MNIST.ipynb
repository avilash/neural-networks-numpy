{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "xgECx1Xe3KJM",
        "Ngy1y1F83Rkn",
        "T2CR1DX86ZtD",
        "3-Lwq1Rk6ekz",
        "79wtTt7-931k",
        "43XhafjD-rd0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avilash/neural-networks-numpy/blob/master/Classification_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_MsdlWs25Gm",
        "colab_type": "text"
      },
      "source": [
        "This notebook trains a feed-forward neural network only in numpy to classify between the 10 digits in the MNIST Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgECx1Xe3KJM",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0z5Mmpz3NTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ngy1y1F83Rkn",
        "colab_type": "text"
      },
      "source": [
        "## Model Forward and Backward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrQ2wW1B3QTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def act(x):\n",
        "  return np.maximum(x, 0.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfPewU1b3fEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def act_back(incoming_gradient, activation):\n",
        "  incoming_gradient[activation == 0.] = 0.\n",
        "  return incoming_gradient"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4WVtXmb3qq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(layer_sizes, input_size, output_size):\n",
        "  model = []\n",
        "  current_input_size = input_size\n",
        "  for i, layer_size in enumerate(layer_sizes):\n",
        "    layer = {}\n",
        "    layer[\"W\"] = np.random.randn(current_input_size, layer_size) / np.sqrt(current_input_size)\n",
        "    layer[\"b\"] = np.zeros(layer_size, dtype=np.float64)\n",
        "    model.append(layer)\n",
        "    current_input_size = layer_size\n",
        "    \n",
        "  layer = {}\n",
        "  layer[\"W\"] = np.random.randn(current_input_size, output_size) / np.sqrt(current_input_size)\n",
        "  layer[\"b\"] = np.zeros(output_size, dtype=np.float64)\n",
        "  model.append(layer)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY6mV_mY3sgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(input, model):\n",
        "  model_len = len(model)\n",
        "    \n",
        "  activations = []\n",
        "  current_act = input\n",
        "  activations.append(current_act)\n",
        " \n",
        "  \n",
        "  for i in range(model_len-1):\n",
        "    layer = model[i]\n",
        "    # print (\"prev_A = {}, W = {}, B = {}\".format(current_act.shape, layer['W'].shape, layer['b'].shape))\n",
        "    current_act = act(np.matmul(current_act, layer['W']) + layer['b'])\n",
        "    # print (\"A = {}\".format(current_act.shape))\n",
        "    activations.append(current_act)\n",
        "\n",
        "  layer = model[model_len-1]\n",
        "  # print (\"prev_A = {}, W = {}, B = {}\".format(current_act.shape, layer['W'].shape, layer['b'].shape))\n",
        "  pred = np.matmul(current_act, layer['W']) + layer['b']\n",
        "  # print (\"A = {}\".format(pred.shape))\n",
        "  activations.append(pred)\n",
        "  \n",
        "  \n",
        "  return activations, pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTZo2Mzn3uXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward(gradient, activations, model):\n",
        "  gradients = []\n",
        "  Z_grad = gradient.copy()\n",
        "    \n",
        "  activation_len = len(activations)\n",
        "  \n",
        "  for i in range(activation_len-2, -1, -1):\n",
        "    grad = {}\n",
        "    prev_A = activations[i]\n",
        "    W = model[i]['W']\n",
        "    grad['W'] = prev_A.T.dot(Z_grad)\n",
        "    grad['b'] = np.sum(Z_grad, axis=0)\n",
        "    grad['A'] = Z_grad.dot(W.T)\n",
        "    gradients.insert(0, grad)\n",
        "    Z_grad = grad['A'].copy()\n",
        "    Z_grad = act_back(Z_grad, prev_A)\n",
        "    \n",
        "  return gradients\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCokjNJrUDwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def probs_from_logits(logits):\n",
        "  probs = np.exp(logits)\n",
        "  probs = probs / (np.sum(probs, axis=1, keepdims=True) + np.finfo(float).eps)\n",
        "  return probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ESs1_SO4qvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ce_loss(y_pred, y):\n",
        "  N = y_pred.shape[0]\n",
        "  probs = probs_from_logits(y_pred)\n",
        "  loss = -np.log(probs[np.arange(N), y])\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXCXTVa95joP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ce_loss_back(y_pred, y):\n",
        "  N = y_pred.shape[0]\n",
        "  mask = np.zeros_like(y_pred)\n",
        "  mask[np.arange(N), y] = 1.\n",
        "  probs = probs_from_logits(y_pred)\n",
        "  loss_back = probs - mask\n",
        "  return loss_back"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2CR1DX86ZtD",
        "colab_type": "text"
      },
      "source": [
        "## Load DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-Lwq1Rk6ekz",
        "colab_type": "text"
      },
      "source": [
        "### MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP4Fpntu6-Rl",
        "colab_type": "text"
      },
      "source": [
        "Code adapted from : https://github.com/hsjeong5/MNIST-for-Numpy/blob/master/mnist.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj0akoXm6dAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib import request\n",
        "import gzip\n",
        "import pickle\n",
        "\n",
        "mnist_filename = [\n",
        "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
        "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
        "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
        "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
        "]\n",
        "\n",
        "def download_mnist():\n",
        "    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
        "    for name in mnist_filename:\n",
        "        print(\"Downloading \"+name[1]+\"...\")\n",
        "        request.urlretrieve(base_url+name[1], name[1])\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "def save_mnist():\n",
        "    mnist = {}\n",
        "    for name in mnist_filename[:2]:\n",
        "        with gzip.open(name[1], 'rb') as f:\n",
        "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
        "    for name in mnist_filename[-2:]:\n",
        "        with gzip.open(name[1], 'rb') as f:\n",
        "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "    with open(\"mnist.pkl\", 'wb') as f:\n",
        "        pickle.dump(mnist,f)\n",
        "    print(\"Save complete.\")\n",
        "    \n",
        "def load():\n",
        "    with open(\"mnist.pkl\",'rb') as f:\n",
        "        mnist = pickle.load(f)\n",
        "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvEZ8cbM7THh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "375c7208-dc1a-48c7-87e6-6296a555b1a5"
      },
      "source": [
        "download_mnist()\n",
        "save_mnist()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train-images-idx3-ubyte.gz...\n",
            "Downloading t10k-images-idx3-ubyte.gz...\n",
            "Downloading train-labels-idx1-ubyte.gz...\n",
            "Downloading t10k-labels-idx1-ubyte.gz...\n",
            "Download complete.\n",
            "Save complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JScwupDq7tQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_full, y_train_full, x_test, y_test = load()\n",
        "indices = np.random.permutation(x_train_full.shape[0])\n",
        "val_idxs, train_idxs = indices[:10000], indices[10000:]\n",
        "x_train, x_val = x_train_full[train_idxs,:], x_train_full[val_idxs,:]\n",
        "y_train, y_val = y_train_full[train_idxs], y_train_full[val_idxs]\n",
        "\n",
        "x_train , x_val, x_test = x_train.astype(np.float64) , x_val.astype(np.float64), x_test.astype(np.float64)\n",
        "mean_image = np.mean(x_train, axis=0).reshape(1, x_train.shape[1])\n",
        "x_train -= mean_image\n",
        "x_val -= mean_image\n",
        "x_test -= mean_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv2YdSiB9xk4",
        "colab_type": "text"
      },
      "source": [
        "## Training Procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79wtTt7-931k",
        "colab_type": "text"
      },
      "source": [
        "### Weight Update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FBwyjht9zzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_weights(model, gradients, lr = 0.001):\n",
        "  for i, grad in enumerate(gradients):\n",
        "    model[i]['W'] -= lr*grad['W']\n",
        "    model[i]['b'] -= lr*grad['b']\n",
        "      \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr2886Yo97Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_rmsprop_params(model):\n",
        "  rmsprop_params = []\n",
        "  for layer in model:\n",
        "    rmsprop_param = {}\n",
        "    rmsprop_param['W'] = np.zeros_like(layer['W'])\n",
        "    rmsprop_param['b'] = np.zeros_like(layer['b'])\n",
        "    rmsprop_params.append(rmsprop_param)\n",
        "  \n",
        "  return rmsprop_params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhU4WgS_-AQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_weights_rmsprop(model, gradients, rmsprop_params, lr = 0.001, beta=0.9):\n",
        "  for i, grad in enumerate(gradients):\n",
        "    v = beta*rmsprop_params[i]['W'] + (1.-beta)*np.square(grad['W'])\n",
        "    model[i]['W'] -= (lr*grad['W'] / np.sqrt(v + 1e-14))\n",
        "    rmsprop_params[i]['W'] = v\n",
        "    \n",
        "    v = beta*rmsprop_params[i]['b'] + (1.-beta)*np.square(grad['b'])\n",
        "    model[i]['b'] -= (lr*grad['b'] / np.sqrt(v + 1e-14))\n",
        "    rmsprop_params[i]['b'] = v\n",
        "      \n",
        "  return model, rmsprop_params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43XhafjD-rd0",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9asIpq83-qUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpwBpPQ--w7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grad_check(model, gradients, x_batch, y_batch):\n",
        "  layer = 0\n",
        "  layer_x = 0\n",
        "  layer_y = 0\n",
        "  grad_num = 0\n",
        "  \n",
        "  x_size, y_size = gradients[layer]['W'].shape\n",
        "  for i in range(x_size):\n",
        "    for j in range(y_size): \n",
        "      grad_num = gradients[layer]['W'][i][j]\n",
        "      if grad_num != 0.:\n",
        "        layer_x = i\n",
        "        layer_y = j\n",
        "        break\n",
        "    if grad_num != 0.:\n",
        "      break\n",
        "\n",
        "  h = 0.00001\n",
        "  oldval = model[layer]['W'][layer_x][layer_y]\n",
        "  model[layer]['W'][layer_x][layer_y] = oldval - h\n",
        "  _ , y_pred1 = forward(x_batch, model)\n",
        "  loss1 = ce_loss(y_pred1, y_batch)\n",
        "  model[layer]['W'][layer_x][layer_y] = oldval + h\n",
        "  _ , y_pred2 = forward(x_batch, model)\n",
        "  loss2 = ce_loss(y_pred2, y_batch)\n",
        "  grad_ana = (loss2 - loss1) / (2.*h)\n",
        "  grad_ana = np.sum(grad_ana)\n",
        "  \n",
        "  model[layer]['W'][layer_x][layer_y] = oldval\n",
        "\n",
        "  error = rel_error(grad_num, grad_ana)\n",
        "  return error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_LLPBLm-3WX",
        "colab_type": "text"
      },
      "source": [
        "### Train / Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du2EQSva-yoi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f53e94a1-57d7-4c71-9e8e-f97767dba2a3"
      },
      "source": [
        "D_in, D_out = x_train.shape[1], 10\n",
        "hidden_layers = [100]\n",
        "model = create_model(hidden_layers, D_in, D_out)\n",
        "rmsprop_params = init_rmsprop_params(model)\n",
        "net_size = len(hidden_layers)\n",
        "\n",
        "train_size = x_train.shape[0]\n",
        "val_size = x_val.shape[0]\n",
        "\n",
        "batch_start = 0\n",
        "batch_end = 0\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "num_batches = int(round(train_size / batch_size))\n",
        "\n",
        "should_grad_check = True\n",
        "if should_grad_check:\n",
        "  x_batch = x_train[0:batch_size]\n",
        "  y_batch = y_train[0:batch_size]\n",
        "  activations, y_pred = forward(x_batch, model)\n",
        "  loss = ce_loss(y_pred, y_batch).mean()\n",
        "  loss_grad = ce_loss_back(y_pred, y_batch)\n",
        "  gradients = backward(loss_grad, activations, model)\n",
        "  err = grad_check(model, gradients, x_batch, y_batch)\n",
        "  print (\"Gradient Check - Rel Error = {} \\n\".format(err))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loss = 0\n",
        "  for i in range(num_batches):\n",
        "    batch_start = (i%num_batches)*batch_size\n",
        "    batch_end = min(train_size, batch_start+batch_size)\n",
        "    x_batch = x_train[batch_start:batch_end]\n",
        "    y_batch = y_train[batch_start:batch_end]\n",
        "\n",
        "    activations, y_pred = forward(x_batch, model)\n",
        "\n",
        "    loss += ce_loss(y_pred, y_batch).mean()\n",
        "\n",
        "    loss_grad = ce_loss_back(y_pred, y_batch)\n",
        "\n",
        "    gradients = backward(loss_grad, activations, model)\n",
        "        \n",
        "    model, rmsprop_params = update_weights_rmsprop(model, gradients, rmsprop_params, lr=0.0001)\n",
        "        \n",
        "    if (i+1)%100 == 0:\n",
        "      print (\"Epoch = {}, Batch = {}, Loss = {}\".format(epoch+1, i+1, loss/100))\n",
        "      loss = 0  \n",
        "    \n",
        "  _, train_pred = forward(x_train, model)\n",
        "  train_acc = np.mean(np.argmax(train_pred, axis=1) == y_train)\n",
        "  _, val_pred = forward(x_val, model)\n",
        "  val_acc = np.mean(np.argmax(val_pred, axis=1) == y_val)\n",
        "  print (\"*** Summary ***\")\n",
        "  print (\"Train Acc = {}, Val Acc = {}\".format(train_acc, val_acc))\n",
        "  print (\"***************\")"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient Check - Rel Error = 3.1573686448861186e-07 \n",
            "\n",
            "Epoch = 1, Batch = 100, Loss = 25.23774951088449\n",
            "Epoch = 1, Batch = 200, Loss = 6.695683753715192\n",
            "Epoch = 1, Batch = 300, Loss = 4.722688101918565\n",
            "Epoch = 1, Batch = 400, Loss = 4.041613333827541\n",
            "Epoch = 1, Batch = 500, Loss = 3.4054990766856714\n",
            "Epoch = 1, Batch = 600, Loss = 2.977395552583612\n",
            "Epoch = 1, Batch = 700, Loss = 2.385888381965591\n",
            "*** Summary ***\n",
            "Train Acc = 0.90254, Val Acc = 0.8919\n",
            "***************\n",
            "Epoch = 2, Batch = 100, Loss = 2.3476988999884973\n",
            "Epoch = 2, Batch = 200, Loss = 1.8982248709422092\n",
            "Epoch = 2, Batch = 300, Loss = 1.7806526036859205\n",
            "Epoch = 2, Batch = 400, Loss = 1.6290206430608414\n",
            "Epoch = 2, Batch = 500, Loss = 1.5681206483870271\n",
            "Epoch = 2, Batch = 600, Loss = 1.495096978874665\n",
            "Epoch = 2, Batch = 700, Loss = 1.2421993890386662\n",
            "*** Summary ***\n",
            "Train Acc = 0.93288, Val Acc = 0.9133\n",
            "***************\n",
            "Epoch = 3, Batch = 100, Loss = 1.3659310341736335\n",
            "Epoch = 3, Batch = 200, Loss = 1.1432392789792416\n",
            "Epoch = 3, Batch = 300, Loss = 1.085281051627835\n",
            "Epoch = 3, Batch = 400, Loss = 0.9912665220643185\n",
            "Epoch = 3, Batch = 500, Loss = 0.9752107304455016\n",
            "Epoch = 3, Batch = 600, Loss = 1.0129287747517555\n",
            "Epoch = 3, Batch = 700, Loss = 0.8353858668650013\n",
            "*** Summary ***\n",
            "Train Acc = 0.94792, Val Acc = 0.919\n",
            "***************\n",
            "Epoch = 4, Batch = 100, Loss = 0.9183100040193374\n",
            "Epoch = 4, Batch = 200, Loss = 0.7573465213393985\n",
            "Epoch = 4, Batch = 300, Loss = 0.7862421140246618\n",
            "Epoch = 4, Batch = 400, Loss = 0.684231180173366\n",
            "Epoch = 4, Batch = 500, Loss = 0.6785450156314303\n",
            "Epoch = 4, Batch = 600, Loss = 0.7311565599125173\n",
            "Epoch = 4, Batch = 700, Loss = 0.5947538256415358\n",
            "*** Summary ***\n",
            "Train Acc = 0.95872, Val Acc = 0.927\n",
            "***************\n",
            "Epoch = 5, Batch = 100, Loss = 0.672106439018289\n",
            "Epoch = 5, Batch = 200, Loss = 0.5345839711138053\n",
            "Epoch = 5, Batch = 300, Loss = 0.5896811153137456\n",
            "Epoch = 5, Batch = 400, Loss = 0.49443359683964805\n",
            "Epoch = 5, Batch = 500, Loss = 0.508163170755026\n",
            "Epoch = 5, Batch = 600, Loss = 0.5510876537730129\n",
            "Epoch = 5, Batch = 700, Loss = 0.43813943784290366\n",
            "*** Summary ***\n",
            "Train Acc = 0.96564, Val Acc = 0.9325\n",
            "***************\n",
            "Epoch = 6, Batch = 100, Loss = 0.4992278751252359\n",
            "Epoch = 6, Batch = 200, Loss = 0.3968969683525614\n",
            "Epoch = 6, Batch = 300, Loss = 0.44557452962424454\n",
            "Epoch = 6, Batch = 400, Loss = 0.3619409100176425\n",
            "Epoch = 6, Batch = 500, Loss = 0.391658570924795\n",
            "Epoch = 6, Batch = 600, Loss = 0.42141200771964626\n",
            "Epoch = 6, Batch = 700, Loss = 0.33967438838413394\n",
            "*** Summary ***\n",
            "Train Acc = 0.97138, Val Acc = 0.9323\n",
            "***************\n",
            "Epoch = 7, Batch = 100, Loss = 0.3873986306991671\n",
            "Epoch = 7, Batch = 200, Loss = 0.29712340544565696\n",
            "Epoch = 7, Batch = 300, Loss = 0.3353975832079714\n",
            "Epoch = 7, Batch = 400, Loss = 0.28082062343127223\n",
            "Epoch = 7, Batch = 500, Loss = 0.3155920413069574\n",
            "Epoch = 7, Batch = 600, Loss = 0.3376063532393508\n",
            "Epoch = 7, Batch = 700, Loss = 0.24539251088601202\n",
            "*** Summary ***\n",
            "Train Acc = 0.97382, Val Acc = 0.9317\n",
            "***************\n",
            "Epoch = 8, Batch = 100, Loss = 0.3038585727691611\n",
            "Epoch = 8, Batch = 200, Loss = 0.23003399239899555\n",
            "Epoch = 8, Batch = 300, Loss = 0.2655887209947612\n",
            "Epoch = 8, Batch = 400, Loss = 0.21328548316992876\n",
            "Epoch = 8, Batch = 500, Loss = 0.24352636324547608\n",
            "Epoch = 8, Batch = 600, Loss = 0.2529487101151059\n",
            "Epoch = 8, Batch = 700, Loss = 0.19080707308101072\n",
            "*** Summary ***\n",
            "Train Acc = 0.97574, Val Acc = 0.934\n",
            "***************\n",
            "Epoch = 9, Batch = 100, Loss = 0.23515163398986666\n",
            "Epoch = 9, Batch = 200, Loss = 0.1779833733925711\n",
            "Epoch = 9, Batch = 300, Loss = 0.21494315333985511\n",
            "Epoch = 9, Batch = 400, Loss = 0.15487423803745753\n",
            "Epoch = 9, Batch = 500, Loss = 0.20093940066463736\n",
            "Epoch = 9, Batch = 600, Loss = 0.20308617633059733\n",
            "Epoch = 9, Batch = 700, Loss = 0.1595873294886011\n",
            "*** Summary ***\n",
            "Train Acc = 0.97824, Val Acc = 0.933\n",
            "***************\n",
            "Epoch = 10, Batch = 100, Loss = 0.17893124234311977\n",
            "Epoch = 10, Batch = 200, Loss = 0.13439379173128232\n",
            "Epoch = 10, Batch = 300, Loss = 0.18835661462529443\n",
            "Epoch = 10, Batch = 400, Loss = 0.12155464492967641\n",
            "Epoch = 10, Batch = 500, Loss = 0.1570970391232369\n",
            "Epoch = 10, Batch = 600, Loss = 0.15291858291858623\n",
            "Epoch = 10, Batch = 700, Loss = 0.12473850371994809\n",
            "*** Summary ***\n",
            "Train Acc = 0.98112, Val Acc = 0.9366\n",
            "***************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2pSvzKcQA7r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "335eeaad-e3ba-498a-803b-d0f4ce4df0fd"
      },
      "source": [
        "_, test_pred = forward(x_test, model)\n",
        "test_acc = np.mean(np.argmax(test_pred, axis=1) == y_test)\n",
        "print (\"Test Acc = {}\".format(test_acc))"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Acc = 0.9433\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}